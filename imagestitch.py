# -*- coding: utf-8 -*-
"""BRUHH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RmPe9pi4W5zPcghYxrDwndRsyL2ajb8I

Import numpy; opencv; and matplotlib
"""

import sys
import cv2
import matplotlib.pyplot as plt
import numpy as np

"""Check Versions."""

print('Python version:', sys.version)
print('OpenCV version:', cv2.__version__)
print('NumPy version: ', np.__version__)

# """Load the two images."""

# from google.colab import drive
# drive.mount('/content/drive')

# import os
# os.chdir('/content/drive/MyDrive/Colab Notebooks/fod code')

# #img1 = cv2.imread('IMG_3921.jpg')

# #img_1 = cv2.imread('IMG_3921.jpg')
# img_1 = cv2.imread('IMG_3845.JPG')
# img_1 = cv2.cvtColor(img_1, cv2.COLOR_BGR2RGB)

# plt.imshow(img_1)
# plt.show()
# #img_2 = cv2.imread('IMG_3922.jpg')
# img_2 = cv2.imread('IMG_3844.JPG')
# img_2 = cv2.cvtColor(img_2, cv2.COLOR_BGR2RGB)

# plt.imshow(img_2)
# plt.show()


def imagestitch(img_1, img_2):
    """Convert images to gray-scale."""

    img1 = cv2.cvtColor(img_1,cv2.COLOR_RGB2GRAY)
    # plt.imshow(img1)
    # plt.show()
    img2 = cv2.cvtColor(img_2,cv2.COLOR_RGB2GRAY)
    # plt.imshow(img2)
    # plt.show()

    """Creating SIFT keypoints and desciptors."""

    sift = cv2.xfeatures2d.SIFT_create()
    kp1, des1 = sift.detectAndCompute(img1,None)
    kp2, des2 = sift.detectAndCompute(img2,None)

    bf = cv2.BFMatcher()

    matches = bf.knnMatch(des1,des2, k=2)

    good = []
    for m in matches:
        if (m[0].distance < 0.5*m[1].distance):
            good.append(m)
    matches = np.asarray(good)

    # Allign the images

    if (len(matches[:,0]) >= 4):
        src = np.float32([ kp1[m.queryIdx].pt for m in matches[:,0] ]).reshape(-1,1,2)
        dst = np.float32([ kp2[m.trainIdx].pt for m in matches[:,0] ]).reshape(-1,1,2)
        H, masked = cv2.findHomography(src, dst, cv2.RANSAC, 5.0)
    else:
        raise AssertionError('Canâ€™t find enough keypoints.')

    """Stitch the Images."""

    dst = cv2.warpPerspective(img_1,H,((img_1.shape[1] + img_2.shape[1]), img_2.shape[0])) #wraped image
    dst[0:img_2.shape[0], 0:img_2.shape[1]] = img_2 #stitched image
    #cv2.imwrite('output.jpg',dst)
    #cv2.imwrite('output.jpg', cv2.cvtColor(dst, cv2.COLOR_BGR2RGB))
    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)
    #plt.imshow(dst)
    #plt.show()

    return dst

#cv2.imshow('Color input image', img)
#cv2.imshow('Histogram equalized', img_output)
# plt.imshow(img_output)
# plt.show()

# TAKES IN MULTIPLE IMAGES

def imagestitchmult(imgs, mode):

    num_pics = len(imgs)
    
    assert num_pics > 1
    
    # Find out if we are using forward or backwards propagation
    if (mode == "forward" or mode == "f"):
        dst = imgs[0]
        for p in range(1, num_pics):
            dst = imagestitch(dst, imgs[p])
    else:
        dst = imgs[num_pics-1]
        for p in range(num_pics-1,0,-1):
            dst = imagestitch(dst, imgs[p])

    return dst